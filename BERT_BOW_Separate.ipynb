{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6054,
     "status": "ok",
     "timestamp": 1588554996683,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "7rWryW65eV5Y",
    "outputId": "6ceba319-6b77-4791-e9b7-ed8a0128df1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.12.47)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.15.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDyJR1DyiOvy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    XLNetConfig, \n",
    "    XLNetForSequenceClassification, \n",
    "    XLNetTokenizer,\n",
    "    XLMConfig, \n",
    "    XLMForSequenceClassification, \n",
    "    XLMTokenizer,\n",
    "    RobertaConfig, \n",
    "    RobertaForSequenceClassification, \n",
    "    RobertaTokenizer,\n",
    "    DistilBertConfig, \n",
    "    DistilBertForSequenceClassification, \n",
    "    DistilBertTokenizer,\n",
    "    AlbertConfig, \n",
    "    AlbertForSequenceClassification, \n",
    "    AlbertTokenizer,\n",
    "    XLMRobertaConfig, \n",
    "    XLMRobertaForSequenceClassification, \n",
    "    XLMRobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I82-yQFQJeTr"
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    \"xlnet\": (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    \"xlm\": (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "    \"albert\": (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer),\n",
    "    \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer),\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLatveM7J1o5"
   },
   "outputs": [],
   "source": [
    "## Model setup\n",
    "model_type = 'bert'\n",
    "model_name = 'bert-base-cased'\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "\n",
    "config = config_class.from_pretrained(model_name, num_labels=50, output_hidden_states=True)\n",
    "model = model_class.from_pretrained(model_name, config=config)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pN-sWy7XUVe3"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "MAX_LEN = 128\n",
    "batch_size = 1\n",
    "lr = 4e-5\n",
    "eps = 1e-8\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99uCftDRiOwB"
   },
   "outputs": [],
   "source": [
    "ROOT = \"\"\n",
    "DATA_PATH = ROOT + 'datasets/'\n",
    "def get_data(subset='train'):\n",
    "    texts = []\n",
    "    for root, folders, files in os.walk(DATA_PATH + '/C50/C50{}'.format(subset)):\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "\n",
    "        author = root.split('/')[-1]\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    texts.append({\n",
    "                        'author': author,\n",
    "                        'text': f.read(),\n",
    "\n",
    "                    })\n",
    "    df = pd.DataFrame(texts)\n",
    "    unique_authors = sorted(df['author'].unique())\n",
    "    num_authors = len(unique_authors)\n",
    "    author_to_id = { unique_authors[i]: i for i in range(num_authors) }\n",
    "    df = df.assign(author_id=df['author'].apply(lambda a: author_to_id[a]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLNmwPLyiOwE"
   },
   "outputs": [],
   "source": [
    "def get_encodings(texts):\n",
    "    token_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        token_id = tokenizer.encode(text, \n",
    "                                    add_special_tokens=True, \n",
    "                                    max_length=MAX_LEN,\n",
    "                                    pad_to_max_length=True)\n",
    "        token_ids.append(token_id)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "\n",
    "def get_attention_masks(padded_encodings):\n",
    "    attention_masks = []\n",
    "    for encoding in padded_encodings:\n",
    "        attention_mask = [int(token_id > 0) for token_id in encoding]\n",
    "        attention_masks.append(attention_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qz4sWj33nTz"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(DATA_PATH + 'reuters50_train.pkl')\n",
    "test_df = pd.read_pickle(DATA_PATH + 'reuters50_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWjVYsuPvaVW"
   },
   "outputs": [],
   "source": [
    "train_encodings = get_encodings(train_df.text.values)\n",
    "train_attention_masks = get_attention_masks(train_encodings)\n",
    "\n",
    "test_encodings = get_encodings(test_df.text.values)\n",
    "test_attention_masks = get_attention_masks(test_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8BfjMuKciOwK"
   },
   "outputs": [],
   "source": [
    "train_input_ids = torch.tensor(train_encodings)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "train_labels = torch.tensor(train_df.author_id.values)\n",
    "\n",
    "\n",
    "test_input_ids = torch.tensor(test_encodings)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "test_labels = torch.tensor(test_df.author_id.values)\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_input_ids, train_masks, train_labels)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(test_input_ids, test_masks, test_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50088,
     "status": "ok",
     "timestamp": 1588555040776,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "cKetuFLPiOwN",
    "outputId": "c4b41140-69b2-4265-9b97-f78c233e7475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCk5nwZyiOwR"
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def get_confusion_matrix(preds, labels):\n",
    "    \"\"\"\n",
    "    Rows = true labels\n",
    "    Columns = classified labels\n",
    "    \"\"\"\n",
    "    confusion_matrix = np.zeros((50, 50))\n",
    "    preds = np.argmax(preds, axis=1).flatten()\n",
    "    labels = labels.flatten()\n",
    "    for i, label in enumerate(labels):\n",
    "        pred = preds[i]\n",
    "        confusion_matrix[label][pred] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "def parse_confusion_matrix(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Rows = labels\n",
    "    Col0 = tp\n",
    "    Col1 = fp\n",
    "    Col2 = fn\n",
    "    Col3 = tn\n",
    "    \"\"\"\n",
    "    parsed_confusion_matrix = np.zeros((50, 4))\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i][i]\n",
    "        fp = confusion_matrix[:, i].sum() - tp\n",
    "        fn = confusion_matrix[i, :].sum() - tp \n",
    "        tn = confusion_matrix.sum() - tp - fp - fn\n",
    "        # print(f'Label: {i}, tp: {tp}, fp: {fp}, fn: {fn}, tn: {tn}')\n",
    "        parsed_confusion_matrix[i][0] = tp\n",
    "        parsed_confusion_matrix[i][1] = fp\n",
    "        parsed_confusion_matrix[i][2] = fn\n",
    "        parsed_confusion_matrix[i][3] = tn\n",
    "    return parsed_confusion_matrix\n",
    "\n",
    "def calculate_avg_precision(parsed_confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculates macro average precision\n",
    "    \"\"\"\n",
    "    total_precision = 0\n",
    "    num_classes = parsed_confusion_matrix.shape[0]\n",
    "    for i in range(num_classes):\n",
    "        tp, fp, _, _ = parsed_confusion_matrix[i]\n",
    "        precision = tp / (tp + fp)\n",
    "        if not np.isnan(precision):\n",
    "            total_precision += precision\n",
    "    return total_precision / num_classes\n",
    "\n",
    "def calculate_avg_recall(parsed_confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculates macro average recall\n",
    "    \"\"\"\n",
    "    total_recall = 0\n",
    "    num_classes = parsed_confusion_matrix.shape[0]\n",
    "    for i in range(num_classes):\n",
    "        tp, _, fn, _ = parsed_confusion_matrix[i]\n",
    "        recall = tp / (tp + fn)\n",
    "        if not np.isnan(recall):\n",
    "            total_recall += recall\n",
    "    return total_recall / num_classes\n",
    "\n",
    "def calculate_avg_f1(parsed_confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculates macro average f1 score\n",
    "    \"\"\"\n",
    "    total_f1 = 0\n",
    "    num_classes = parsed_confusion_matrix.shape[0]\n",
    "    for i in range(num_classes):\n",
    "        tp, fp, fn, _ = parsed_confusion_matrix[i]\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2*((precision * recall) / (precision + recall))\n",
    "        if not np.isnan(f1):\n",
    "            total_f1 += f1\n",
    "    return total_f1 / num_classes\n",
    "\n",
    "def calculate_avg_mcc(parsed_confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculates macro average Matthews correlation coefficient\n",
    "    \"\"\"\n",
    "    total_mcc = 0\n",
    "    num_classes = parsed_confusion_matrix.shape[0]\n",
    "    for i in range(num_classes):\n",
    "        tp, fp, fn, tn = parsed_confusion_matrix[i]\n",
    "        mcc = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        if not np.isnan(mcc):\n",
    "            total_mcc += mcc\n",
    "    return total_mcc / num_classes\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ReZQjKdn8e-"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#         Get Training Embeddings\n",
    "# ========================================\n",
    "\n",
    "model.eval()\n",
    "train_features = None\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    b_texts = batch[0].to(device)\n",
    "    b_attention_masks = batch[1].to(device)\n",
    "    b_authors = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_texts, \n",
    "                        attention_mask=b_attention_masks, \n",
    "                        labels=b_authors)\n",
    "    \n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    hidden_states = outputs[2][0]\n",
    "\n",
    "    if train_features is None:\n",
    "        train_features = hidden_states\n",
    "    else:\n",
    "        train_features = torch.cat((train_features, hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1JjmFssiOwV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "#         Get Validation Embeddings\n",
    "# ========================================\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_features = None\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    \n",
    "    b_texts = batch[0].to(device)\n",
    "    b_attention_masks = batch[1].to(device)\n",
    "    b_authors = batch[2].to(device)\n",
    "   \n",
    "    with torch.no_grad():        \n",
    "\n",
    "        outputs = model(b_texts, \n",
    "                        attention_mask=b_attention_masks,\n",
    "                        labels=b_authors\n",
    "                        )\n",
    "\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    hidden_states = outputs[2][0]\n",
    "\n",
    "    if test_features is None:\n",
    "        test_features = hidden_states\n",
    "    else:\n",
    "        test_features = torch.cat((test_features, hidden_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 144968,
     "status": "ok",
     "timestamp": 1588555135677,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "sRJzZFAfX-g4",
    "outputId": "11440bd7-0b19-48f0-86c5-116ea3792639"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4500, 768])"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJkaBYUAgtoi"
   },
   "outputs": [],
   "source": [
    "train_features = train_features.mean(dim=1).double().to(device)\n",
    "test_features = test_features.mean(dim=1).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqVkEgIjkPg5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_xbow_p = DATA_PATH + \"text_bow_train.p\"\n",
    "test_xbow_p = DATA_PATH + \"text_bow_test.p\"\n",
    "train_bow = pickle.load(open(train_xbow_p, \"rb\"))\n",
    "test_bow = pickle.load(open(test_xbow_p, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hY01HB01lbvd"
   },
   "outputs": [],
   "source": [
    "# train_bow = torch.from_numpy(train_bow)\n",
    "# test_bow = torch.from_numpy(test_bow)\n",
    "train_bow = torch.tensor(train_bow.todense(), dtype = torch.double, device = device)\n",
    "test_bow = torch.tensor(test_bow.todense(), dtype = torch.double, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfG7H84PmW0j"
   },
   "outputs": [],
   "source": [
    "train_features_comb = torch.cat((train_features, train_bow), dim=1)\n",
    "test_features_comb = torch.cat((test_features, test_bow), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYowUQaBrzmW"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(815, 700)\n",
    "        # self.hidden2 = nn.Linear(600, 600)\n",
    "        # self.hidden3 = nn.Linear(192, 96)\n",
    "        self.output = nn.Linear(700, 50)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # self.batchnorm1 = nn.BatchNorm1d(300)\n",
    "        # self.batchnorm2 = nn.BatchNorm1d(100)\n",
    "        # self.batchnorm3 = nn.BatchNorm1d(96)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # x = self.hidden2(x)\n",
    "        # x = self.batchnorm2(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = self.hidden3(x)\n",
    "        # x = self.batchnorm3(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "       \n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 144923,
     "status": "ok",
     "timestamp": 1588555135681,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "3jApuPL9wEBc",
    "outputId": "18e7cc33-2d8c-4a4a-c87f-4b2d63a6818b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (hidden1): Linear(in_features=815, out_features=700, bias=True)\n",
       "  (output): Linear(in_features=700, out_features=50, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = NeuralNet()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nnet.parameters(), lr=0.001)\n",
    "\n",
    "nnet.double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYPmDrPXwNXu"
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_features_comb, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "validation_data = TensorDataset(test_features_comb, test_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5X18U6JFw3ei"
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    nnet.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in train_dataloader:\n",
    "        features = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = nnet(features)\n",
    "        vals, inds = torch.max(outputs, dim=1)\n",
    "        \n",
    "        loss = loss_function(outputs, labels)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        acc = torch.eq(inds, labels).sum().item() / labels.shape[0]\n",
    "        epoch_acc += acc\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    avg_epoch_acc = epoch_acc / len(train_dataloader)\n",
    "    print(f\"Epoch: [{epoch}/{epochs}], loss: {avg_epoch_loss}, acc: {avg_epoch_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 145502,
     "status": "ok",
     "timestamp": 1588555136281,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "tm0ucoAcx1BD",
    "outputId": "2e974d2b-10e5-4465-9196-0ec85eac64ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.12200262460178901\n",
      "Loss: 1.1559330537118295\n",
      "Loss: 1.9726935365004608\n",
      "Loss: 0.7785658032196777\n",
      "Loss: 0.24049036764612924\n",
      "Loss: 1.5836146557194524\n",
      "Loss: 0.5428488692304398\n",
      "Loss: 0.5594733800538633\n",
      "Loss: 0.9071051551107013\n",
      "Loss: 0.7174421776457237\n",
      "Loss: 0.9718123500830679\n",
      "Loss: 1.0838673740193756\n",
      "Loss: 1.0315812517406668\n",
      "Loss: 0.4610786831195107\n",
      "Loss: 0.5711651786702145\n",
      "Loss: 0.9163085083728522\n",
      "Acc: 0.821484375\n"
     ]
    }
   ],
   "source": [
    "nnet.eval()\n",
    "total_acc = 0\n",
    "preds = []\n",
    "trues = []\n",
    "for batch in validation_dataloader:\n",
    "    features = batch[0].to(device)\n",
    "    labels = batch[1].to(device)\n",
    "    trues.extend(labels)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = nnet(features)\n",
    "        vals, inds = torch.max(outputs, dim=1)\n",
    "        preds.extend(inds)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        print(f\"Loss: {loss}\")\n",
    "    acc = torch.eq(inds, labels).sum().item() / labels.shape[0]\n",
    "    total_acc += acc\n",
    "avg_acc = total_acc / len(validation_dataloader)\n",
    "\n",
    "print(f'Acc: {avg_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zE9nNuKSyu3g"
   },
   "outputs": [],
   "source": [
    "pred_array = np.zeros(len(preds))\n",
    "gold = np.zeros(len(preds))\n",
    "for i in range(len(trues)):\n",
    "    pred_array[i] = int(preds[i].cpu().numpy())\n",
    "    gold[i] = trues[i].cpu().numpy()\n",
    "pred_array = pred_array.astype(int)\n",
    "gold = gold.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWcHlFRqy5gz"
   },
   "outputs": [],
   "source": [
    "confmatrix = np.zeros((50, 50))\n",
    "for i, label in enumerate(gold):\n",
    "    pred = preds[i]\n",
    "    confmatrix[label][pred] += 1\n",
    "parsed_confmatrix = parse_confusion_matrix(confmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 145758,
     "status": "ok",
     "timestamp": 1588555136552,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "hg08XVPR2WXU",
    "outputId": "3992b698-3187-45eb-b729-ff960db8f05b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8201109393067259"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg_f1(parsed_confmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 145752,
     "status": "ok",
     "timestamp": 1588555136552,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "OGQN-P1g3k7d",
    "outputId": "a8c77ed4-9db1-4e63-8a18-56f698dc7185"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821169843706492"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg_mcc(parsed_confmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 145747,
     "status": "ok",
     "timestamp": 1588555136553,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "5s6GSTyh_BTA",
    "outputId": "84a29dc4-bb6a-4241-c050-b96a36189efa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.836338873544756"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg_precision(parsed_confmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 145741,
     "status": "ok",
     "timestamp": 1588555136553,
     "user": {
      "displayName": "Worthan Kwan",
      "photoUrl": "",
      "userId": "03671344327906115243"
     },
     "user_tz": 240
    },
    "id": "ejpPKrkl1B9m",
    "outputId": "f4ef4c73-b019-47d9-a66b-17b72bde5765"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8219999999999997"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg_recall(parsed_confmatrix)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_BOW_SEPERATE_EMBEDDINGS_CLASSIFICATION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
